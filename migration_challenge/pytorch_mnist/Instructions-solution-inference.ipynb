{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker 로 MNIST 모델 추론\n",
    "\n",
    "### 실험 환경\n",
    "- [SageMaker Notebook Instance](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html) 의 pytorch_p310 커널에서 테스트 되었습니다.\n",
    "#### 참고 문서\n",
    "- [[Module 2.1] 인퍼런스 스크래치](https://github.com/aws-samples/aws-ai-ml-workshop-kr/blob/master/sagemaker/recommendation/Neural-Collaborative-Filtering-On-SageMaker/2_Inference/2.1.NCF-Inference-Scratch.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이전 훈련 노트북에서 모델 저장 경로 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_s3_path: \n",
      " s3://sagemaker-us-east-1-057716757052/sagemaker-101-pytorch-2024-06-23-02-17--2024-06-23-02-17-36-523/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "%store -r model_s3_path\n",
    "\n",
    "print(\"model_s3_path: \\n\", model_s3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# External Dependencies:\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Local Notebook Utils:\n",
    "# import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 로컬 추론 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 아티펙트 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model_dir = 'local_model'\n",
    "os.makedirs(local_model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-057716757052/sagemaker-101-pytorch-2024-06-23-02-17--2024-06-23-02-17-36-523/output/model.tar.gz to local_model/model.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tar: Ignoring unknown extended header keyword `LIBARCHIVE.creationtime'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.pth\n"
     ]
    }
   ],
   "source": [
    "%%sh -s {model_s3_path} {local_model_dir}\n",
    "\n",
    "model_s3_path=$1\n",
    "local_model_dir=$2\n",
    "# 모델을 S3에서 로컬로 다운로드\n",
    "aws s3 cp $model_s3_path $local_model_dir\n",
    "\n",
    "# 모델 다운로드 폴더로 이동\n",
    "cd $local_model_dir\n",
    "\n",
    "# 압축 해제\n",
    "tar -xvf model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.추론 함수 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 로딩 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device: \", device)\n",
    "\n",
    "local_model_dir = 'local_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Model is successfully loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    '''\n",
    "    Load a model\n",
    "    '''\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = torch.jit.load(os.path.join(model_dir, 'model.pth'))\n",
    "    model = model.to(device)\n",
    "    print(\"## Model is successfully loaded\")\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = model_fn(local_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### payload 생성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "from PIL import Image\n",
    "\n",
    "def create_payload(label, testing_dir):\n",
    "    filename = os.listdir(f\"{testing_dir}/{label}\")[0]\n",
    "    print(\"filename: \", filename)\n",
    "\n",
    "    # Load the image:\n",
    "    img = Image.open(f\"{testing_dir}/{label}/{filename}\")\n",
    "    # normalize\n",
    "    input_data = np.asarray(img).astype(np.float32)\n",
    "    # input_data = np.expand_dims(input_data, [0, 1])  # Add batch & leading channel dim\n",
    "    input_data = input_data.flatten().tolist()\n",
    "    print(\"input_data: \", np.shape(input_data))\n",
    "\n",
    "    payload = {\n",
    "        'input': input_data, # input_data\n",
    "        'resolution' : [28, 28]\n",
    "    }\n",
    "\n",
    "    return json.dumps(payload)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename:  995.png\n",
      "input_data:  (784,)\n",
      "payload: \n",
      " {\"input\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 63.0, 255.0, 253.0, 232.0, 109.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 144.0, 253.0, 252.0, 252.0, 252.0, 156.0, 10.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6.0, 160.0, 253.0, 252.0, 252.0, 252.0, 253.0, 190.0, 51.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 37.0, 252.0, 253.0, 252.0, 252.0, 252.0, 253.0, 252.0, 154.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 37.0, 252.0, 253.0, 35.0, 21.0, 205.0, 253.0, 252.0, 252.0, 252.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 37.0, 252.0, 253.0, 35.0, 0.0, 104.0, 211.0, 252.0, 252.0, 252.0, 156.0, 10.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.0, 154.0, 72.0, 10.0, 0.0, 0.0, 31.0, 206.0, 252.0, 252.0, 253.0, 138.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.0, 211.0, 252.0, 253.0, 179.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 120.0, 253.0, 255.0, 211.0, 31.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 37.0, 252.0, 253.0, 252.0, 71.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 37.0, 252.0, 253.0, 241.0, 61.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 42.0, 62.0, 144.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 120.0, 252.0, 253.0, 179.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 218.0, 253.0, 253.0, 191.0, 78.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 32.0, 212.0, 253.0, 255.0, 180.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 94.0, 247.0, 252.0, 252.0, 252.0, 242.0, 217.0, 134.0, 0.0, 0.0, 0.0, 0.0, 32.0, 207.0, 252.0, 252.0, 253.0, 200.0, 63.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 109.0, 252.0, 252.0, 252.0, 252.0, 252.0, 252.0, 252.0, 182.0, 181.0, 78.0, 37.0, 212.0, 252.0, 252.0, 252.0, 253.0, 252.0, 221.0, 16.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 109.0, 252.0, 252.0, 252.0, 169.0, 252.0, 252.0, 252.0, 253.0, 252.0, 252.0, 252.0, 253.0, 252.0, 252.0, 252.0, 253.0, 252.0, 231.0, 46.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 110.0, 253.0, 253.0, 253.0, 109.0, 150.0, 253.0, 253.0, 255.0, 253.0, 253.0, 253.0, 255.0, 253.0, 253.0, 253.0, 255.0, 253.0, 154.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 78.0, 242.0, 252.0, 252.0, 252.0, 252.0, 252.0, 252.0, 253.0, 252.0, 252.0, 252.0, 253.0, 252.0, 252.0, 252.0, 180.0, 138.0, 10.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.0, 201.0, 252.0, 252.0, 252.0, 252.0, 252.0, 253.0, 220.0, 195.0, 71.0, 72.0, 71.0, 71.0, 71.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 78.0, 108.0, 252.0, 210.0, 108.0, 108.0, 108.0, 15.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], \"resolution\": [28, 28]}\n"
     ]
    }
   ],
   "source": [
    "local_dir = \"/tmp/mnist\"\n",
    "testing_dir = f\"{local_dir}/testing\"\n",
    "payload = create_payload(label=2, testing_dir=testing_dir)\n",
    "print(\"payload: \\n\", payload)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input_fn 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################\n",
      "## Starting Input_fn\n",
      "###############################\n",
      "## content_type:  application/json\n",
      "## data: \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 63.0, 255.0, 253.0, 232.0, 109.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 144.0, 253.0, 252.0, 252.0, 252.0, 156.0, 10.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6.0, 160.0, 253.0, 252.0, 252.0, 252.0, 253.0, 190.0, 51.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 37.0, 252.0, 253.0, 252.0, 252.0, 252.0, 253.0, 252.0, 154.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 37.0, 252.0, 253.0, 35.0, 21.0, 205.0, 253.0, 252.0, 252.0, 252.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 37.0, 252.0, 253.0, 35.0, 0.0, 104.0, 211.0, 252.0, 252.0, 252.0, 156.0, 10.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.0, 154.0, 72.0, 10.0, 0.0, 0.0, 31.0, 206.0, 252.0, 252.0, 253.0, 138.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.0, 211.0, 252.0, 253.0, 179.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 120.0, 253.0, 255.0, 211.0, 31.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 37.0, 252.0, 253.0, 252.0, 71.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 37.0, 252.0, 253.0, 241.0, 61.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 42.0, 62.0, 144.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 120.0, 252.0, 253.0, 179.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 218.0, 253.0, 253.0, 191.0, 78.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 32.0, 212.0, 253.0, 255.0, 180.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 94.0, 247.0, 252.0, 252.0, 252.0, 242.0, 217.0, 134.0, 0.0, 0.0, 0.0, 0.0, 32.0, 207.0, 252.0, 252.0, 253.0, 200.0, 63.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 109.0, 252.0, 252.0, 252.0, 252.0, 252.0, 252.0, 252.0, 182.0, 181.0, 78.0, 37.0, 212.0, 252.0, 252.0, 252.0, 253.0, 252.0, 221.0, 16.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 109.0, 252.0, 252.0, 252.0, 169.0, 252.0, 252.0, 252.0, 253.0, 252.0, 252.0, 252.0, 253.0, 252.0, 252.0, 252.0, 253.0, 252.0, 231.0, 46.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 110.0, 253.0, 253.0, 253.0, 109.0, 150.0, 253.0, 253.0, 255.0, 253.0, 253.0, 253.0, 255.0, 253.0, 253.0, 253.0, 255.0, 253.0, 154.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 78.0, 242.0, 252.0, 252.0, 252.0, 252.0, 252.0, 252.0, 253.0, 252.0, 252.0, 252.0, 253.0, 252.0, 252.0, 252.0, 180.0, 138.0, 10.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 62.0, 201.0, 252.0, 252.0, 252.0, 252.0, 252.0, 253.0, 220.0, 195.0, 71.0, 72.0, 71.0, 71.0, 71.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 78.0, 108.0, 252.0, 210.0, 108.0, 108.0, 108.0, 15.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "## np.shape:  torch.Size([1, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import io\n",
    "\n",
    "def input_fn(input_data, content_type):\n",
    "    print(\"###############################\")    \n",
    "    print(\"## Starting Input_fn\")\n",
    "    print(\"###############################\")   \n",
    "\n",
    "    try: \n",
    "        if content_type == 'application/json':\n",
    "            print(\"## content_type: \", content_type)\n",
    "            # image data in the form of list type ( size: 784 = 28 * 28 )\n",
    "\n",
    "            if isinstance(input_data, str):\n",
    "                pass\n",
    "            elif isinstance(input_data, io.BytesIO):\n",
    "                print(\"## io.BytesIO\")\n",
    "                input_data = input_data.read()\n",
    "                input_data = bytes.decode(input_data)        \n",
    "            elif isinstance(input_data, bytes):\n",
    "                print(\"## bytes:\")                \n",
    "                input_data = input_data.decode()\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported data type: {type(input_data)}\")        \n",
    "\n",
    "            # print(\"## before json loads \")\n",
    "            # print(\"## data type : \\n\", type(data))\n",
    "            data = json.loads(input_data)[\"input\"]\n",
    "            # print(\"## after json loads : \\n\", data)\n",
    "            # Get resolution ( [28, 28])\n",
    "            resolution = json.loads(input_data)[\"resolution\"]\n",
    "            # print(\"## data: \\n\", data)\n",
    "            \n",
    "            # convert one dimenstion to two dimentions: 784 --> (28, 28)\n",
    "            data = np.array(data).reshape(resolution)\n",
    "            # normalize\n",
    "            data = np.squeeze(data).astype(np.float32) / 255\n",
    "            # (28, 28 ) --> (1, 1, 28, 28)\n",
    "            data = torch.tensor(data).unsqueeze(0).unsqueeze(0)    \n",
    "            print(\"## np.shape: \", np.shape(data))\n",
    "        else:\n",
    "            print(\"################################\")\n",
    "            raise ValueError(f\"Unsupported content type: {content_type}\")        \n",
    "            print(\"################################\")\n",
    "    except Exception:\n",
    "        print(traceback.format_exc())        \n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "def input_fn(input_data, content_type):\n",
    "    print(\"###############################\")    \n",
    "    print(\"## Starting Input_fn\")\n",
    "    print(\"###############################\")   \n",
    "\n",
    "    try: \n",
    "        if content_type == 'application/json':\n",
    "            print(\"## content_type: \", content_type)\n",
    "            # image data in the form of list type ( size: 784 = 28 * 28 )\n",
    "\n",
    "            if isinstance(input_data, str):\n",
    "                pass\n",
    "            elif isinstance(input_data, io.BytesIO):\n",
    "                input_data = input_data.read()\n",
    "                input_data = bytes.decode(input_data)        \n",
    "            elif isinstance(input_data, bytes):\n",
    "                input_data = input_data.decode()\n",
    "\n",
    "            data = json.loads(input_data)[\"input\"]\n",
    "            # Get resolution ( [28, 28])\n",
    "            resolution = json.loads(input_data)[\"resolution\"]\n",
    "            print(\"## data: \\n\", data)\n",
    "            \n",
    "            # convert one dimenstion to two dimentions: 784 --> (28, 28)\n",
    "            data = np.array(data).reshape(resolution)\n",
    "            # normalize\n",
    "            data = np.squeeze(data).astype(np.float32) / 255\n",
    "            # (28, 28 ) --> (1, 1, 28, 28)\n",
    "            data = torch.tensor(data).unsqueeze(0).unsqueeze(0)    \n",
    "            print(\"## np.shape: \", np.shape(data))\n",
    "        else:\n",
    "            print(\"################################\")\n",
    "            raise ValueError(f\"Unsupported content type: {content_type}\")        \n",
    "            print(\"################################\")\n",
    "    except Exception:\n",
    "        print(traceback.format_exc())  \n",
    "\n",
    "    return data    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_data = input_fn(input_data=payload, content_type='application/json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict_fn 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([1, 1, 28, 28])\n",
      "## Result confidences: tensor([[1.1233e-05, 6.4257e-10, 9.9999e-01, 9.8468e-08, 2.1759e-11, 1.7181e-12,\n",
      "         9.0501e-11, 1.1180e-08, 7.9277e-10, 6.0489e-11]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def predict_fn(input_data, model):\n",
    "    '''\n",
    "    모델의 추론 함수\n",
    "    '''\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    input_data = input_data.to(device)\n",
    "    # values = input_data.squeeze().tolist()\n",
    "    # print(\"input sum: \", np.sum(values))\n",
    "    print(\"shape: \", np.shape(input_data))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        result = model(input_data)\n",
    "    print(f\"## Result confidences: {result}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "result = predict_fn(input_data=input_data, model=model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output_fn 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_fn(prediction, content_type):\n",
    "\n",
    "    if content_type == 'application/json':\n",
    "        # convert tensor type to list type\n",
    "        values = prediction.squeeze().tolist()\n",
    "\n",
    "    return json.dumps(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:  [1.1233007171540521e-05, 6.42566944097922e-10, 0.9999886751174927, 9.846751680697707e-08, 2.1758643498070995e-11, 1.718074575836337e-12, 9.050087068640522e-11, 1.1180278924882714e-08, 7.927744571922801e-10, 6.048894024557327e-11]\n",
      "## Max confidence index: 2\n"
     ]
    }
   ],
   "source": [
    "output = output_fn(prediction=result, content_type='application/json')\n",
    "output = json.loads(output)\n",
    "print(\"output: \", output)\n",
    "\n",
    "\n",
    "max_index = np.argmax(output)\n",
    "print(\"## Max confidence index:\", max_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전체 로컬 추론 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def predict_mnist(label, local_model_dir, testing_dir ):\n",
    "\n",
    "    # Handle input\n",
    "    # Load the image:\n",
    "    filename = os.listdir(f\"{testing_dir}/{label}\")[0]\n",
    "    print(\"previous filename: \", filename)\n",
    "    img = Image.open(f\"{testing_dir}/{label}/{filename}\")\n",
    "\n",
    "    # load model\n",
    "    model = model_fn(local_model_dir)\n",
    "\n",
    "    # load input_data\n",
    "    payload = create_payload(label=label, testing_dir=testing_dir)\n",
    "    # print(\"## label: \", label)\n",
    "    # print(\"## payload: \\n\", payload)    \n",
    "    input_data = input_fn(input_data=payload, content_type='application/json')\n",
    "\n",
    "    # print(\"## input_data shape: \", np.shape(input_data))\n",
    "\n",
    "    # predict label\n",
    "    result = predict_fn(input_data=input_data, model=model)\n",
    "\n",
    "    # output \n",
    "    output = output_fn(prediction=result, content_type='application/json')\n",
    "    output = json.loads(output)    \n",
    "    # print(\"## output: \", output)\n",
    "\n",
    "    max_index = np.argmax(output)\n",
    "    print(\"## Max confidence index:\", max_index)    \n",
    "    \n",
    "    # Plot the result:\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    fig = plt.subplot(1, 1, 1)\n",
    "    ax = plt.imshow(img, cmap=\"gray\")\n",
    "    fig.set_title(f\"Predicted Number {max_index}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous filename:  5479.png\n",
      "## Model is successfully loaded\n",
      "filename:  5479.png\n",
      "input_data:  (784,)\n",
      "###############################\n",
      "## Starting Input_fn\n",
      "###############################\n",
      "## content_type:  application/json\n",
      "## data: \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 84.0, 120.0, 164.0, 254.0, 254.0, 254.0, 157.0, 47.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 117.0, 245.0, 254.0, 254.0, 254.0, 254.0, 254.0, 254.0, 219.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 137.0, 254.0, 254.0, 254.0, 199.0, 175.0, 205.0, 254.0, 248.0, 54.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 26.0, 192.0, 95.0, 19.0, 6.0, 22.0, 160.0, 254.0, 243.0, 36.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 69.0, 220.0, 254.0, 254.0, 158.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 82.0, 164.0, 254.0, 254.0, 254.0, 196.0, 15.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 26.0, 155.0, 254.0, 254.0, 254.0, 250.0, 158.0, 15.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 85.0, 218.0, 254.0, 254.0, 254.0, 254.0, 226.0, 49.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 176.0, 254.0, 254.0, 254.0, 254.0, 254.0, 254.0, 246.0, 175.0, 92.0, 12.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 176.0, 254.0, 254.0, 232.0, 163.0, 185.0, 155.0, 231.0, 254.0, 254.0, 153.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.0, 143.0, 61.0, 0.0, 0.0, 0.0, 0.0, 0.0, 122.0, 246.0, 224.0, 12.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0, 218.0, 254.0, 65.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 59.0, 254.0, 254.0, 49.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.0, 78.0, 202.0, 254.0, 241.0, 16.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 152.0, 254.0, 254.0, 246.0, 70.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 73.0, 127.0, 5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0, 157.0, 254.0, 254.0, 242.0, 68.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 118.0, 254.0, 150.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 43.0, 203.0, 254.0, 254.0, 208.0, 106.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 44.0, 230.0, 254.0, 224.0, 177.0, 131.0, 79.0, 131.0, 176.0, 254.0, 254.0, 254.0, 166.0, 48.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 44.0, 228.0, 254.0, 255.0, 254.0, 254.0, 254.0, 254.0, 255.0, 250.0, 117.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 32.0, 133.0, 179.0, 254.0, 254.0, 254.0, 178.0, 156.0, 69.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "## np.shape:  torch.Size([1, 1, 28, 28])\n",
      "shape:  torch.Size([1, 1, 28, 28])\n",
      "## Result confidences: tensor([[3.5067e-14, 3.1423e-10, 4.4553e-13, 1.0000e+00, 3.2764e-16, 4.0042e-07,\n",
      "         6.4458e-14, 6.8528e-12, 1.9019e-12, 2.3170e-10]], device='cuda:0')\n",
      "## Max confidence index: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "predict_mnist(label=3, local_model_dir=local_model_dir, testing_dir=testing_dir )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous filename:  3501.png\n",
      "## Model is successfully loaded\n",
      "filename:  3501.png\n",
      "input_data:  (784,)\n",
      "###############################\n",
      "## Starting Input_fn\n",
      "###############################\n",
      "## content_type:  application/json\n",
      "## data: \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 16.0, 97.0, 168.0, 180.0, 254.0, 255.0, 254.0, 219.0, 74.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 87.0, 239.0, 254.0, 254.0, 190.0, 176.0, 176.0, 182.0, 254.0, 239.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 133.0, 253.0, 174.0, 51.0, 8.0, 2.0, 0.0, 0.0, 22.0, 254.0, 241.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 40.0, 205.0, 252.0, 175.0, 14.0, 0.0, 0.0, 0.0, 0.0, 0.0, 46.0, 254.0, 241.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 64.0, 236.0, 238.0, 122.0, 2.0, 0.0, 0.0, 0.0, 0.0, 8.0, 104.0, 224.0, 254.0, 160.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 43.0, 241.0, 238.0, 60.0, 0.0, 0.0, 0.0, 11.0, 61.0, 142.0, 235.0, 254.0, 254.0, 254.0, 154.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 74.0, 254.0, 159.0, 0.0, 0.0, 0.0, 139.0, 233.0, 254.0, 254.0, 247.0, 159.0, 172.0, 254.0, 107.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 59.0, 251.0, 224.0, 138.0, 157.0, 224.0, 240.0, 202.0, 159.0, 105.0, 26.0, 0.0, 184.0, 244.0, 32.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 109.0, 218.0, 242.0, 206.0, 188.0, 39.0, 0.0, 0.0, 0.0, 0.0, 23.0, 246.0, 236.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10.0, 29.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 26.0, 254.0, 236.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 69.0, 254.0, 164.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 113.0, 251.0, 72.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 152.0, 232.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 24.0, 242.0, 232.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.0, 254.0, 195.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.0, 254.0, 146.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.0, 254.0, 146.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 63.0, 254.0, 85.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 117.0, 254.0, 60.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 117.0, 254.0, 60.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "## np.shape:  torch.Size([1, 1, 28, 28])\n",
      "shape:  torch.Size([1, 1, 28, 28])\n",
      "## Result confidences: tensor([[8.6908e-12, 1.3907e-11, 3.7165e-10, 1.6102e-08, 2.3171e-08, 2.9803e-10,\n",
      "         6.0593e-16, 2.0275e-05, 7.5478e-09, 9.9998e-01]], device='cuda:0')\n",
      "## Max confidence index: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_mnist(label=9, local_model_dir=local_model_dir, testing_dir=testing_dir )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Local 에서 SageMaker Endpoint 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_model_path:  local_model/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "local_model_path = os.path.join(local_model_dir, 'model.tar.gz')\n",
    "print(\"local_model_path: \", local_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instance_type 을 local_gpu or local 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance type = local_gpu\n"
     ]
    }
   ],
   "source": [
    "instance_type = 'local_gpu' if torch.cuda.is_available() else 'local'\n",
    "print(\"Instance type = \" + instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sagemaker.pytorch.model.PyTorchModel 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/SageMaker/.xdg/config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import sagemaker\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "endpoint_name = \"local-endpoint-mnist-{}\".format(int(time.time()))\n",
    "\n",
    "local_pytorch_model = PyTorchModel(model_data=local_model_path,\n",
    "                                   role=role,\n",
    "                                   entry_point='inference.py',\n",
    "                                   source_dir = 'src',\n",
    "                                   framework_version='2.0.1',\n",
    "                                   py_version='py310',\n",
    "                                   model_server_workers=1,\n",
    "                                  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 로컬 SageMaker Endpoint 배포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to p9b20mvuwr-algo-1-jyvao\n",
      "p9b20mvuwr-algo-1-jyvao  | ['torchserve', '--start', '--model-store', '/.sagemaker/ts/models', '--ts-config', '/etc/sagemaker-ts.properties', '--log-config', '/opt/conda/lib/python3.10/site-packages/sagemaker_pytorch_serving_container/etc/log4j2.xml', '--models', 'model=/opt/ml/model']\n",
      "p9b20mvuwr-algo-1-jyvao  | Warning: TorchServe is using non-default JVM parameters: -XX:-UseContainerSupport\n",
      "p9b20mvuwr-algo-1-jyvao  | WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:05,270 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:05,273 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:05,346 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:05,524 [INFO ] main org.pytorch.serve.ModelServer - \n",
      "p9b20mvuwr-algo-1-jyvao  | Torchserve version: 0.8.2\n",
      "p9b20mvuwr-algo-1-jyvao  | TS Home: /opt/conda/lib/python3.10/site-packages\n",
      "p9b20mvuwr-algo-1-jyvao  | Current directory: /\n",
      "p9b20mvuwr-algo-1-jyvao  | Temp directory: /home/model-server/tmp\n",
      "p9b20mvuwr-algo-1-jyvao  | Metrics config path: /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml\n",
      "p9b20mvuwr-algo-1-jyvao  | Number of GPUs: 4\n",
      "p9b20mvuwr-algo-1-jyvao  | Number of CPUs: 32\n",
      "p9b20mvuwr-algo-1-jyvao  | Max heap size: 30688 M\n",
      "p9b20mvuwr-algo-1-jyvao  | Python executable: /opt/conda/bin/python3.10\n",
      "p9b20mvuwr-algo-1-jyvao  | Config file: /etc/sagemaker-ts.properties\n",
      "p9b20mvuwr-algo-1-jyvao  | Inference address: http://0.0.0.0:8080\n",
      "p9b20mvuwr-algo-1-jyvao  | Management address: http://0.0.0.0:8080\n",
      "p9b20mvuwr-algo-1-jyvao  | Metrics address: http://127.0.0.1:8082\n",
      "p9b20mvuwr-algo-1-jyvao  | Model Store: /.sagemaker/ts/models\n",
      "p9b20mvuwr-algo-1-jyvao  | Initial Models: model=/opt/ml/model\n",
      "p9b20mvuwr-algo-1-jyvao  | Log dir: /logs\n",
      "p9b20mvuwr-algo-1-jyvao  | Metrics dir: /logs\n",
      "p9b20mvuwr-algo-1-jyvao  | Netty threads: 0\n",
      "p9b20mvuwr-algo-1-jyvao  | Netty client threads: 0\n",
      "p9b20mvuwr-algo-1-jyvao  | Default workers per model: 1\n",
      "p9b20mvuwr-algo-1-jyvao  | Blacklist Regex: N/A\n",
      "p9b20mvuwr-algo-1-jyvao  | Maximum Response Size: 6553500\n",
      "p9b20mvuwr-algo-1-jyvao  | Maximum Request Size: 6553500\n",
      "p9b20mvuwr-algo-1-jyvao  | Limit Maximum Image Pixels: true\n",
      "p9b20mvuwr-algo-1-jyvao  | Prefer direct buffer: false\n",
      "p9b20mvuwr-algo-1-jyvao  | Allowed Urls: [file://.*|http(s)?://.*]\n",
      "p9b20mvuwr-algo-1-jyvao  | Custom python dependency for model allowed: false\n",
      "p9b20mvuwr-algo-1-jyvao  | Enable metrics API: true\n",
      "p9b20mvuwr-algo-1-jyvao  | Metrics mode: log\n",
      "p9b20mvuwr-algo-1-jyvao  | Disable system metrics: true\n",
      "p9b20mvuwr-algo-1-jyvao  | Workflow Store: /.sagemaker/ts/models\n",
      "p9b20mvuwr-algo-1-jyvao  | Model config: N/A\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:05,532 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:05,559 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: /opt/ml/model\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:05,563 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:05,564 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:05,567 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:05,577 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:05,646 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:05,647 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:05,648 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\n",
      "p9b20mvuwr-algo-1-jyvao  | Model server started.\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:07,009 [INFO ] pool-2-thread-2 ACCESS_LOG - /172.19.0.1:52838 \"GET /ping HTTP/1.1\" 200 11\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:07,011 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:7c5b5652efca,timestamp:1719131167\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:07,222 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/home/model-server/tmp/.ts.sock, s_name_part1=9000, pid=102\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:07,224 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:07,233 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.10/site-packages/ts/configs/metrics.yaml.\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:07,234 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]102\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:07,234 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:07,234 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.10.9\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:07,238 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:07,243 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:07,246 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD to backend at: 1719131167246\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:07,269 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:07,770 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 502\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:07,771 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:2197.0|#WorkerName:W-9000-model_1.0,Level:Host|#hostname:7c5b5652efca,timestamp:1719131167\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:07,771 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:23.0|#Level:Host|#hostname:7c5b5652efca,timestamp:1719131167\n",
      "!"
     ]
    }
   ],
   "source": [
    "local_predictor = local_pytorch_model.deploy(\n",
    "                           instance_type=instance_type, \n",
    "                           initial_instance_count=1, \n",
    "                           endpoint_name=endpoint_name,\n",
    "                           wait=True,\n",
    "                           log = False,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 로컬 엔드포인트 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### payload 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename:  5479.png\n",
      "input_data:  (784,)\n",
      "## payload: \n",
      " {\"input\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 84.0, 120.0, 164.0, 254.0, 254.0, 254.0, 157.0, 47.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 117.0, 245.0, 254.0, 254.0, 254.0, 254.0, 254.0, 254.0, 219.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 137.0, 254.0, 254.0, 254.0, 199.0, 175.0, 205.0, 254.0, 248.0, 54.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 26.0, 192.0, 95.0, 19.0, 6.0, 22.0, 160.0, 254.0, 243.0, 36.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 69.0, 220.0, 254.0, 254.0, 158.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 82.0, 164.0, 254.0, 254.0, 254.0, 196.0, 15.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 26.0, 155.0, 254.0, 254.0, 254.0, 250.0, 158.0, 15.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 85.0, 218.0, 254.0, 254.0, 254.0, 254.0, 226.0, 49.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 176.0, 254.0, 254.0, 254.0, 254.0, 254.0, 254.0, 246.0, 175.0, 92.0, 12.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 176.0, 254.0, 254.0, 232.0, 163.0, 185.0, 155.0, 231.0, 254.0, 254.0, 153.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.0, 143.0, 61.0, 0.0, 0.0, 0.0, 0.0, 0.0, 122.0, 246.0, 224.0, 12.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0, 218.0, 254.0, 65.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 59.0, 254.0, 254.0, 49.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.0, 78.0, 202.0, 254.0, 241.0, 16.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 152.0, 254.0, 254.0, 246.0, 70.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 73.0, 127.0, 5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0, 157.0, 254.0, 254.0, 242.0, 68.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 118.0, 254.0, 150.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 43.0, 203.0, 254.0, 254.0, 208.0, 106.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 44.0, 230.0, 254.0, 224.0, 177.0, 131.0, 79.0, 131.0, 176.0, 254.0, 254.0, 254.0, 166.0, 48.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 44.0, 228.0, 254.0, 255.0, 254.0, 254.0, 254.0, 254.0, 255.0, 250.0, 117.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 32.0, 133.0, 179.0, 254.0, 254.0, 254.0, 178.0, 156.0, 69.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], \"resolution\": [28, 28]}\n"
     ]
    }
   ],
   "source": [
    "test_label = 3\n",
    "payload = create_payload(label= test_label, testing_dir=testing_dir)\n",
    "print(\"## payload: \\n\", payload)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sagemaker runtime_client 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime_client:  <sagemaker.local.local_session.LocalSagemakerRuntimeClient object at 0x7fbca21ade10>\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "def get_sm_runtime_client(instance_type):\n",
    "    if instance_type in ['local_gpu', 'local']:\n",
    "        runtime_client = sagemaker.local.LocalSagemakerRuntimeClient()    \n",
    "        print(\"runtime_client: \", runtime_client)\n",
    "    else:\n",
    "        runtime_client = boto3.Session().client('sagemaker-runtime')\n",
    "        print(\"runtime_client: \", runtime_client)    \n",
    "\n",
    "    return runtime_client        \n",
    "\n",
    "\n",
    "runtime_client = get_sm_runtime_client(instance_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### boto3 invoke_endpoint() 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:38,549 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:7c5b5652efca,timestamp:1719131198\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:38,551 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd PREDICT to backend at: 1719131198551\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:38,554 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - ###############################\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:38,555 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - ## Model is successfully loaded\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:38,555 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - ###############################\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:38,556 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1719131198\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:38,918 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - ###############################\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:38,919 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - ## Starting Input_fn\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:38,919 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - ###############################\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:38,919 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - ## content_type:  application/json\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:38,919 [INFO ] W-9000-model_1.0 ACCESS_LOG - /172.19.0.1:40828 \"POST /invocations HTTP/1.1\" 200 375\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:38,920 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - ## np.shape:  torch.Size([1, 1, 28, 28])\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:38,920 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:7c5b5652efca,timestamp:1719131198\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:38,920 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - ###############################\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:38,921 [INFO ] W-9000-model_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:367701.94|#model_name:model,model_version:default|#hostname:7c5b5652efca,timestamp:1719131198\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:38,921 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - ## Starting Predict_fn\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:38,921 [INFO ] W-9000-model_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:161.232|#model_name:model,model_version:default|#hostname:7c5b5652efca,timestamp:1719131198\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:38,922 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - ###############################\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:38,922 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.Milliseconds:0.0|#Level:Host|#hostname:7c5b5652efca,timestamp:1719131198\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:38,923 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - shape:  torch.Size([1, 1, 28, 28])\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:38,923 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 366\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:38,923 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:6.0|#Level:Host|#hostname:7c5b5652efca,timestamp:1719131198\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:38,923 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - ## Result confidences: tensor([[3.5067e-14, 3.1423e-10, 4.4553e-13, 1.0000e+00, 3.2764e-16, 4.0042e-07,\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:38,924 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -          6.4458e-14, 6.8528e-12, 1.9019e-12, 2.3170e-10]], device='cuda:0')\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:38,924 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - ###############################\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:38,924 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - ## Starting Output_fn\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:38,925 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - ###############################\n",
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:38,925 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:363.73|#ModelName:model,Level:Model|#hostname:7c5b5652efca,1719131198,3564911c-29d5-4322-a730-b22cc8553f4f, pattern=[METRICS]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def boto3_invoke_endpoint(runtime_client, endpoint_name, payload):\n",
    "\n",
    "        response = runtime_client.invoke_endpoint(\n",
    "                EndpointName=endpoint_name, \n",
    "                ContentType='application/json', \n",
    "                Body=payload,\n",
    "                )\n",
    "\n",
    "        return response\n",
    "\n",
    "response = boto3_invoke_endpoint(runtime_client, endpoint_name, payload)        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p9b20mvuwr-algo-1-jyvao  | 2024-06-23T08:26:38,928 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.ms:363.73|#ModelName:model,Level:Model|#hostname:7c5b5652efca,requestID:3564911c-29d5-4322-a730-b22cc8553f4f,timestamp:1719131198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:  ['[3.5067052207978996e-14, 3.142335835981669e-10, 4.4553388213984524e-13, 0.9999996423721313, 3.2763954377801503e-16, 4.0042363025349914e-07, 6.445760131158426e-14, 6.852785266325823e-12, 1.901922846644921e-12, 2.3169910434717167e-10]']\n",
      "output:  [3.5067052207978996e-14, 3.142335835981669e-10, 4.4553388213984524e-13, 0.9999996423721313, 3.2763954377801503e-16, 4.0042363025349914e-07, 6.445760131158426e-14, 6.852785266325823e-12, 1.901922846644921e-12, 2.3169910434717167e-10]\n",
      "## Max confidence index: 3\n"
     ]
    }
   ],
   "source": [
    "def parse_output(response):\n",
    "    result = response['Body'].read().decode().splitlines()                            \n",
    "    print('result: ', result)\n",
    "    output = json.loads(result[0])\n",
    "    print('output: ', output)\n",
    "\n",
    "    max_index = np.argmax(output)\n",
    "    print(\"## Max confidence index:\", max_index)    \n",
    "    \n",
    "parse_output(response)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 로컬 엔드포인트 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Start\n",
      "--- Deleted model: pytorch-inference-2024-06-23-08-26-01-526\n",
      "--- Deleted endpoint: local-endpoint-mnist-1719131160\n",
      "--- Deleted endpoint_config: local-endpoint-mnist-1719131160\n"
     ]
    }
   ],
   "source": [
    "def delete_endpoint(client, endpoint_name):\n",
    "    print(\"#### Start\")\n",
    "    response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    EndpointConfigName = response['EndpointConfigName']    \n",
    "    \n",
    "    response = client.describe_endpoint_config(EndpointConfigName=EndpointConfigName)\n",
    "    \n",
    "    model_name = response['ProductionVariants'][0]['ModelName']\n",
    "\n",
    "\n",
    "    print(f'--- Deleted model: {model_name}')\n",
    "    print(f'--- Deleted endpoint: {endpoint_name}')\n",
    "    print(f'--- Deleted endpoint_config: {EndpointConfigName}')    \n",
    "    \n",
    "    client.delete_model(ModelName=model_name)    \n",
    "    client.delete_endpoint_config(EndpointConfigName=EndpointConfigName)        \n",
    "    client.delete_endpoint(EndpointName=endpoint_name)\n",
    "\n",
    "client = sagemaker.local.LocalSagemakerClient()\n",
    "delete_endpoint(client, endpoint_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. SageMaker Endpoint 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = 'ml.m5.xlarge' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "sm_endpoint_name = \"sm-endpoint-mnist-{}\".format(int(time.time()))\n",
    "\n",
    "sm_pytorch_model = PyTorchModel(model_data=local_model_path,\n",
    "                                   role=role,\n",
    "                                   entry_point='inference.py',\n",
    "                                   source_dir = 'src',\n",
    "                                   framework_version='2.0.1',\n",
    "                                   py_version='py310',\n",
    "                                   model_server_workers=1,\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "sm_predictor = sm_pytorch_model.deploy(\n",
    "                           instance_type=instance_type, \n",
    "                           initial_instance_count=1, \n",
    "                           endpoint_name=sm_endpoint_name,\n",
    "                           wait=True,\n",
    "                           log = False,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. SageMaker Endpoint 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename:  5479.png\n",
      "input_data:  (784,)\n",
      "## payload: \n",
      " {\"input\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 84.0, 120.0, 164.0, 254.0, 254.0, 254.0, 157.0, 47.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 117.0, 245.0, 254.0, 254.0, 254.0, 254.0, 254.0, 254.0, 219.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 137.0, 254.0, 254.0, 254.0, 199.0, 175.0, 205.0, 254.0, 248.0, 54.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 26.0, 192.0, 95.0, 19.0, 6.0, 22.0, 160.0, 254.0, 243.0, 36.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 69.0, 220.0, 254.0, 254.0, 158.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 82.0, 164.0, 254.0, 254.0, 254.0, 196.0, 15.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 26.0, 155.0, 254.0, 254.0, 254.0, 250.0, 158.0, 15.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 85.0, 218.0, 254.0, 254.0, 254.0, 254.0, 226.0, 49.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 176.0, 254.0, 254.0, 254.0, 254.0, 254.0, 254.0, 246.0, 175.0, 92.0, 12.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 176.0, 254.0, 254.0, 232.0, 163.0, 185.0, 155.0, 231.0, 254.0, 254.0, 153.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.0, 143.0, 61.0, 0.0, 0.0, 0.0, 0.0, 0.0, 122.0, 246.0, 224.0, 12.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0, 218.0, 254.0, 65.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 59.0, 254.0, 254.0, 49.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.0, 78.0, 202.0, 254.0, 241.0, 16.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 152.0, 254.0, 254.0, 246.0, 70.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 73.0, 127.0, 5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 5.0, 157.0, 254.0, 254.0, 242.0, 68.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 118.0, 254.0, 150.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 43.0, 203.0, 254.0, 254.0, 208.0, 106.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 44.0, 230.0, 254.0, 224.0, 177.0, 131.0, 79.0, 131.0, 176.0, 254.0, 254.0, 254.0, 166.0, 48.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 44.0, 228.0, 254.0, 255.0, 254.0, 254.0, 254.0, 254.0, 255.0, 250.0, 117.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 32.0, 133.0, 179.0, 254.0, 254.0, 254.0, 178.0, 156.0, 69.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], \"resolution\": [28, 28]}\n",
      "runtime_client:  <botocore.client.SageMakerRuntime object at 0x7fbc11867f70>\n",
      "result:  ['[3.5067052207978996e-14, 3.1423419422083043e-10, 4.455355897582669e-13, 0.9999996423721313, 3.276420319372976e-16, 4.0042476712187636e-07, 6.445772328432867e-14, 6.8527982767518925e-12, 1.9019265329323076e-12, 2.3169997864780356e-10]']\n",
      "output:  [3.5067052207978996e-14, 3.1423419422083043e-10, 4.455355897582669e-13, 0.9999996423721313, 3.276420319372976e-16, 4.0042476712187636e-07, 6.445772328432867e-14, 6.8527982767518925e-12, 1.9019265329323076e-12, 2.3169997864780356e-10]\n",
      "## Max confidence index: 3\n"
     ]
    }
   ],
   "source": [
    "test_label = 3\n",
    "payload = create_payload(label= test_label, testing_dir=testing_dir)\n",
    "print(\"## payload: \\n\", payload)    \n",
    "\n",
    "\n",
    "runtime_client = get_sm_runtime_client(instance_type)\n",
    "response = boto3_invoke_endpoint(runtime_client, sm_endpoint_name, payload)        \n",
    "parse_output(response)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. SageMaker Endpoint 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Start\n",
      "--- Deleted model: pytorch-inference-2024-06-23-08-26-52-875\n",
      "--- Deleted endpoint: sm-endpoint-mnist-1719131211\n",
      "--- Deleted endpoint_config: sm-endpoint-mnist-1719131211\n"
     ]
    }
   ],
   "source": [
    "client = boto3.Session().client('sagemaker')\n",
    "delete_endpoint(client,sm_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "pytorch_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
